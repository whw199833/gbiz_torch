{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from gbiz_torch.model import CANModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input = torch.randn((8, 3, 5))\n",
    "item_input = torch.randn((8, 24))\n",
    "\n",
    "test_input = (seq_input, item_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without context input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 1])\n",
      "test_output is tensor([[[ 0.1276],\n",
      "         [-0.2776],\n",
      "         [ 1.4480]],\n",
      "\n",
      "        [[-0.2107],\n",
      "         [-0.0582],\n",
      "         [ 0.0662]],\n",
      "\n",
      "        [[ 0.1039],\n",
      "         [-0.3805],\n",
      "         [ 1.3931]],\n",
      "\n",
      "        [[ 2.6283],\n",
      "         [ 6.7156],\n",
      "         [-0.2936]],\n",
      "\n",
      "        [[ 0.0249],\n",
      "         [-0.2275],\n",
      "         [ 0.2396]],\n",
      "\n",
      "        [[ 0.0666],\n",
      "         [ 0.1325],\n",
      "         [ 0.0793]],\n",
      "\n",
      "        [[-0.0755],\n",
      "         [ 0.4626],\n",
      "         [-0.3609]],\n",
      "\n",
      "        [[ 0.5758],\n",
      "         [-0.2569],\n",
      "         [ 0.4778]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "(bz, seq_lenght, dim_a) = seq_input.shape\n",
    "(bz, dim_b) = item_input.shape\n",
    "\n",
    "in_shape_list=[dim_a, dim_b]\n",
    "hidden_units=[4]\n",
    "projection_in_shape=hidden_units[-1]\n",
    "\n",
    "CAN_Model = CANModel(in_shape_list=in_shape_list, hidden_units=hidden_units, projection_in_shape=projection_in_shape)\n",
    "\n",
    "test_output = CAN_Model(test_input)\n",
    "\n",
    "print(test_output.shape)\n",
    "print(f\"test_output is {test_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with context input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.2273e-01, -2.0360e-01, -1.2115e+00, -8.2793e-01, -8.8206e-01,\n",
       "           2.3839e-01],\n",
       "         [ 1.3453e+00, -2.2837e-01,  8.7825e-02, -3.9904e-01,  1.7411e-01,\n",
       "           1.7135e+00],\n",
       "         [-9.9337e-01,  5.8293e-01,  1.2241e+00, -6.2794e-01,  1.3275e+00,\n",
       "          -6.8095e-01]],\n",
       "\n",
       "        [[-1.5280e+00, -6.7133e-01,  4.9932e-01, -1.5894e+00, -7.7618e-01,\n",
       "           2.4800e-01],\n",
       "         [ 8.7124e-01, -1.0385e+00, -2.0289e-01,  1.5020e+00,  1.3339e+00,\n",
       "          -9.5291e-01],\n",
       "         [ 1.2376e+00, -1.3587e-03,  9.1218e-02, -1.3288e+00, -6.9961e-01,\n",
       "          -5.3976e-01]],\n",
       "\n",
       "        [[-5.6981e-01, -5.4979e-01,  5.2494e-01, -1.4259e+00,  2.8934e-01,\n",
       "          -7.3402e-03],\n",
       "         [-9.4783e-01,  1.6551e+00, -3.0936e+00, -1.2919e+00,  1.1834e+00,\n",
       "           1.5259e+00],\n",
       "         [-8.7503e-01,  4.5146e-03,  1.5245e+00,  1.3136e+00,  3.3647e-01,\n",
       "           1.1253e+00]],\n",
       "\n",
       "        [[ 2.1226e+00, -7.3115e-01,  7.2912e-01, -2.6714e-01, -1.3548e+00,\n",
       "          -1.1558e+00],\n",
       "         [ 9.2304e-01, -9.1052e-01, -9.6086e-01, -7.6549e-01, -4.2516e-01,\n",
       "          -7.4486e-01],\n",
       "         [-1.0879e+00,  3.1781e-01, -1.5326e-01,  3.9459e-01, -5.9435e-01,\n",
       "          -3.8566e-01]],\n",
       "\n",
       "        [[ 1.7417e+00, -6.7344e-01, -1.3049e+00, -6.9970e-01,  4.6877e-01,\n",
       "           3.5815e-01],\n",
       "         [-9.9183e-01,  9.9495e-03, -1.3194e-01, -2.6343e-01, -1.1371e+00,\n",
       "           5.9127e-01],\n",
       "         [-8.4969e-01, -5.3160e-02,  4.4969e-01, -1.4741e+00,  4.7344e-01,\n",
       "          -2.3935e+00]],\n",
       "\n",
       "        [[ 8.3411e-02, -4.1531e-01, -1.4128e+00, -7.6788e-01,  1.3393e+00,\n",
       "           5.9073e-02],\n",
       "         [-6.5676e-01, -2.4702e-01, -5.6949e-01,  8.1295e-01,  1.1607e+00,\n",
       "           7.3864e-01],\n",
       "         [-4.7963e-01, -1.1770e+00, -1.4252e+00,  1.2533e+00, -1.3392e+00,\n",
       "           1.9510e+00]],\n",
       "\n",
       "        [[ 5.7541e-01,  1.0904e+00, -4.8231e-02,  6.7059e-01, -4.1939e-01,\n",
       "           1.1839e+00],\n",
       "         [-1.9539e-01, -1.4840e-01,  3.7384e-01,  6.1227e-01,  2.5752e+00,\n",
       "          -1.1515e-01],\n",
       "         [-4.0196e-01, -1.1568e+00,  3.8729e-01, -5.2836e-01,  1.1410e+00,\n",
       "           9.0964e-01]],\n",
       "\n",
       "        [[-8.7169e-01, -4.6099e-02,  6.6077e-01,  7.9894e-01,  4.3027e-01,\n",
       "           1.2898e+00],\n",
       "         [ 7.2504e-01,  8.6434e-01,  2.1156e+00,  6.5884e-02, -3.4273e-01,\n",
       "           1.7562e-01],\n",
       "         [-6.0442e-01,  9.4726e-01, -9.8471e-01, -4.7950e-01, -5.5288e-01,\n",
       "           7.1112e-01]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(8,3,4)\n",
    "b = torch.randn(8,3,2)\n",
    "torch.cat([a,b], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 1])\n",
      "test_output is tensor([[[ 2.8102],\n",
      "         [-0.1079],\n",
      "         [ 0.3976]],\n",
      "\n",
      "        [[ 0.9769],\n",
      "         [ 0.3283],\n",
      "         [-0.2162]],\n",
      "\n",
      "        [[-0.2122],\n",
      "         [-0.1519],\n",
      "         [-0.6821]],\n",
      "\n",
      "        [[ 2.4320],\n",
      "         [-0.5140],\n",
      "         [-1.1717]],\n",
      "\n",
      "        [[ 1.3750],\n",
      "         [-0.5036],\n",
      "         [-0.1870]],\n",
      "\n",
      "        [[-0.0048],\n",
      "         [ 0.8310],\n",
      "         [ 0.7821]],\n",
      "\n",
      "        [[-0.1839],\n",
      "         [-0.1738],\n",
      "         [-0.1547]],\n",
      "\n",
      "        [[-1.3783],\n",
      "         [-0.1091],\n",
      "         [-0.1640]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "extra_input = torch.randn(8,2)\n",
    "(bz, ex_dim) = extra_input.shape\n",
    "(bz, seq_lenght, dim_a) = seq_input.shape\n",
    "(bz, dim_b) = item_input.shape\n",
    "\n",
    "in_shape_list=[dim_a, dim_b]\n",
    "hidden_units=[4]\n",
    "projection_in_shape=hidden_units[-1]+ex_dim\n",
    "\n",
    "CAN_Model = CANModel(in_shape_list=in_shape_list, hidden_units=hidden_units, projection_in_shape=projection_in_shape)\n",
    "\n",
    "test_output = CAN_Model(test_input, extra_input)\n",
    "\n",
    "print(test_output.shape)\n",
    "print(f\"test_output is {test_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
