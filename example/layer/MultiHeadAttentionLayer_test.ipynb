{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gbiz_torch.layer import MultiHeadAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = torch.randint(0, 2, (8, 3))\n",
    "# padding_idx = 0\n",
    "embedding = nn.Embedding(2, 6)\n",
    "test_input = embedding(test_input)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape is torch.Size([8, 3, 4])\n",
      "test_output is tensor([[[-0.2018, -0.6482, -0.2472, -0.2291],\n",
      "         [ 0.3457, -0.4916,  0.6033, -0.2482],\n",
      "         [ 0.9923, -0.2036, -0.3489,  0.2261]],\n",
      "\n",
      "        [[-0.1717, -0.5038,  0.0234, -0.1179],\n",
      "         [ 0.3535, -0.4946,  0.5244, -0.2088],\n",
      "         [ 1.0664, -0.3279, -0.4342,  0.0702]],\n",
      "\n",
      "        [[-0.1340,  0.0319,  0.5484,  0.5515],\n",
      "         [ 0.4477, -0.3225,  1.1308, -0.1616],\n",
      "         [ 1.3173, -0.0987, -0.1371,  0.3438]],\n",
      "\n",
      "        [[-0.2241, -0.3885,  0.3970, -0.0729],\n",
      "         [ 0.0223, -0.7591, -0.2617, -0.3689],\n",
      "         [ 1.2720, -0.1320, -0.1744,  0.3141]],\n",
      "\n",
      "        [[-0.1314, -0.7264, -0.5385, -0.2267],\n",
      "         [ 0.0606, -0.7177, -0.3528, -0.2733],\n",
      "         [ 0.4779, -0.5550, -0.6959, -0.0234]],\n",
      "\n",
      "        [[-0.3090, -0.6675,  0.1080, -0.4213],\n",
      "         [ 0.1809, -0.5469, -0.0053, -0.1045],\n",
      "         [ 1.2720, -0.1320, -0.1744,  0.3141]],\n",
      "\n",
      "        [[-0.1406, -0.2146,  0.3009,  0.2435],\n",
      "         [ 0.4606, -0.4753,  0.1692, -0.0165],\n",
      "         [ 0.8833, -0.2011,  0.0245,  0.2689]],\n",
      "\n",
      "        [[-0.1283, -0.2195,  0.1748,  0.3065],\n",
      "         [-0.0354, -0.6625, -0.1955, -0.2478],\n",
      "         [ 1.0461, -0.1436, -0.1797,  0.4001]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"test_input.shape is {test_input.shape}\")\n",
    "## n_inputs: n_fields\n",
    "## in_shape: emb_dim\n",
    "\n",
    "MHA_layer = MultiHeadAttentionLayer(in_shape=6, hidden_units=4, heads=2)\n",
    "test_output = MHA_layer(test_input)\n",
    "\n",
    "print(f\"test_output.shape is {test_output.shape}\")\n",
    "print(f\"test_output is {test_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape is torch.Size([8, 3, 4])\n",
      "test_output is tensor([[[ 0.1079,  1.0583, -1.8477,  0.7582],\n",
      "         [-0.4440,  0.2964,  0.9272,  0.6584],\n",
      "         [-0.8281,  0.1667, -0.3840,  0.4245]],\n",
      "\n",
      "        [[ 0.2613,  1.1384, -1.5564,  0.8650],\n",
      "         [-0.9164,  0.5820,  0.7079,  0.8497],\n",
      "         [-0.8294,  1.1883, -0.6341,  1.1876]],\n",
      "\n",
      "        [[ 0.3156,  0.9427, -1.7852,  0.6424],\n",
      "         [-0.2879,  1.4795,  1.9117,  1.8504],\n",
      "         [-0.3620, -1.1714, -0.0469, -0.6664]],\n",
      "\n",
      "        [[-0.2158,  0.8466, -0.5540,  0.8506],\n",
      "         [-0.7218,  1.3020,  0.8682,  1.4534],\n",
      "         [-0.5938,  0.1040, -1.1017,  0.0982]],\n",
      "\n",
      "        [[-0.2150,  0.1706, -0.7923,  0.2741],\n",
      "         [-0.8618,  0.9762,  1.2936,  1.2839],\n",
      "         [ 0.2221, -0.8105, -2.0413, -0.8272]],\n",
      "\n",
      "        [[-0.1528,  0.5273, -0.8962,  0.5216],\n",
      "         [-0.5995,  0.5495,  1.0636,  0.8913],\n",
      "         [-0.9575,  0.0796,  0.2082,  0.4880]],\n",
      "\n",
      "        [[-0.1552,  0.4157, -1.0318,  0.4190],\n",
      "         [-0.6147,  1.3207,  1.2897,  1.4518],\n",
      "         [-0.0371, -1.1801, -0.4152, -0.7947]],\n",
      "\n",
      "        [[ 0.0025,  0.6532, -1.2272,  0.5325],\n",
      "         [-0.1994,  0.4986,  0.3719,  0.5983],\n",
      "         [-0.1026, -0.8751, -0.9586, -0.6624]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "(bz, seq, dim)= test_input.shape\n",
    "in_shape = dim\n",
    "hidden_units = 4\n",
    "heads = 4\n",
    "mask = torch.randint(0, 2, (bz, heads, seq, seq))\n",
    "\n",
    "MHA_layer = MultiHeadAttentionLayer(in_shape=in_shape, hidden_units=hidden_units, heads=heads)\n",
    "test_output = MHA_layer(test_input, mask=mask)\n",
    "\n",
    "print(f\"test_output.shape is {test_output.shape}\")\n",
    "print(f\"test_output is {test_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
